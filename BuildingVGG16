import os, shutil
import tensorflow as tf

from keras.applications import VGG16
from keras.layers import Dense, Input, Flatten, Dropout
from keras.layers import Activation

from keras.optimizers import RMSprop, SGD
from keras import models
from keras import layers
from keras.preprocessing import image
from keras.layers import Dropout
from keras.applications.vgg19 import preprocess_input
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
from keras.callbacks import ModelCheckpoint
from keras import optimizers

from scipy import misc
import matplotlib.pyplot as plt
import numpy as np
import keras as K
import dataset as dataset
from keras.utils import to_categorical
base_dir = 'C:\\Users\\Yashar\\Desktop\\training_data'
testing_dir = 'C:\\Users\\Yashar\\Desktop\\training_data\\Testing'
training_dir = 'C:\\Users\\Yashar\\Desktop\\training_data\\Training'
validation_dir = 'C:\\Users\\Yashar\\Desktop\\training_data\\Validation'


image_list = []

# Build the dataset
classnames = ["C1H","C2H","C1L","C2L","C1M","S1L","URML","URMM","W1","W2"]
for i in classnames:
    dir_list = os.listdir(os.path.join(base_dir,i))
    print("class " + str(i) + " has " + str(len((dir_list))) + " images")
    image_list.extend(dir_list)
    
    
print("total images: " + str(len(image_list)))

# build the model
data = dataset.read_train_sets(base_dir,220,["W1","W2"],validation_size=.4)

trainData = data.train.images
trainLabels = data.train.labels
valData = data.valid.images
valLabels = data.valid.labels

print(trainData.shape)
print(trainLabels.shape)
print(valData.shape)
print(valLabels.shape)

with tf.device('/gpu:1'):


	conv_base = VGG16(weights = 'imagenet',include_top = False, input_shape = (220,220,3),classes = 10)
	conv_base.trainable = False

	model = models.Sequential()
	model.add(conv_base)
	model.add(Flatten(name='flatten_1'))
	model.add(Dense(4096))	# first dense layer 
	model.add(Activation('relu'))
#model.add(Dropout(0.33))
	model.add(Dense(4096))  # 2nd dense layer
	model.add(Activation('relu'))
	model.add(Dense(1000))# 3rd dense layer
	model.add(Activation('relu'))
	model.add(Dense(2))
	model.add(Activation('softmax'))
	
#for layers in conv_base:
#	model.add(layers)

	print(model.summary())

# Train the model
	model.compile(SGD(lr=.0001),loss = 'categorical_crossentropy', metrics = ['accuracy'])
	print("trainable",model.trainable_weights)

	datagen = ImageDataGenerator(
		rotation_range=2,
		width_shift_range=0.25,
		height_shift_range=.05,
		shear_range=.2,
		zoom_range=0.8,
		horizontal_flip=True,
		fill_mode='nearest'
	)

	train_gen = datagen.flow(trainData, trainLabels, batch_size=16)

	mout = model.fit_generator(generator=train_gen, steps_per_epoch=trainData.shape[0] // 16, epochs=5,
                               verbose=1, validation_data=(valData, valLabels))




loss = mout.history['loss']
val_loss = mout.history['val_loss']

epochs = range(1, len(loss) + 1)
plt.subplot(1,2,1)
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1,2,2)
acc = mout.history['acc']
val_acc = mout.history['val_acc']
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
